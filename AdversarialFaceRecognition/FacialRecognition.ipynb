{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Testing Facial Recognition with Adversarial Examples\n",
    "By Emily Strong\n",
    "\n",
    "## Introduction - The OpenFace Facial Recognition Model\n",
    "\n",
    "## Adversarial Examples\n",
    "\n",
    "## Validating the Pipeline\n",
    "OpenFace comes with demo code for face classification. One of the examples included is of Steve Carell shown below. OpenFace uses older versions of several Python libraries so running the script returns warnings about depreciated features. I have suppressed these to make the output easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ./images/examples/carell.jpg ===\n",
      "Predict SteveCarell with 0.99 confidence.\n"
     ]
    }
   ],
   "source": [
    "%run ./demos/classifier.py infer models/openface/celeb-classifier.nn4.small2.v1.pkl ./images/examples/carell.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can run this same script with a model I have trained on my own data to identify an image of Keira Knightley. In this project I am using two models, one trained only on the people used as subjects for my test cases, and one trained on the full FaceScrub data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== test-images/keira_knightley3.jpg ===\n",
      "Predict keira-knightley with 0.99 confidence.\n"
     ]
    }
   ],
   "source": [
    "%run ./demos/classifier.py infer ./generated-embeddings/limited/classifier.pkl test-images/keira_knightley3.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== test-images/keira_knightley3.jpg ===\n",
      "Predict keira-knightley with 0.81 confidence.\n"
     ]
    }
   ],
   "source": [
    "%run ./demos/classifier.py infer ./generated-embeddings/fullfacescrub/classifier.pkl test-images/keira_knightley3.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Image Augmentation\n",
    "In the first part of this project I am stress testing the classification model by augmenting the image in ways that should decrease the image quality or otherwise make it more difficult to obtain the measurements that OpenFace uses to identify the faces.\n",
    "\n",
    "### Face Classification Workflow\n",
    "I need to adapt the portions of \"/demos/classifyer.py\" used for testing a trained model so that it can be called within a while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import pandas as pd\n",
    "\n",
    "import openface\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the paths for the models used in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(os.path.realpath('FacialRecognition.ipynb'))\n",
    "#modelDir = os.path.join(fileDir, '..', 'models')\n",
    "modelDir = os.path.join(fileDir, 'models')\n",
    "dlibModelDir = os.path.join(modelDir, 'dlib')\n",
    "openfaceModelDir = os.path.join(modelDir, 'openface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the path of the OpenFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/openface/models/openface'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openfaceModelDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "align = openface.AlignDlib(os.path.join(dlibModelDir,\"shape_predictor_68_face_landmarks.dat\"))\n",
    "net = openface.TorchNeuralNet(os.path.join(openfaceModelDir,'nn4.small2.v1.t7'), imgDim=96)\n",
    "                           \n",
    "def getRep(imgPath):\n",
    "\n",
    "    bgrImg = cv2.imread(imgPath)\n",
    "    if bgrImg is None:\n",
    "        raise Exception(\"Unable to load image: {}\".format(imgPath))\n",
    "\n",
    "    rgbImg = cv2.cvtColor(bgrImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "    bb1 = align.getLargestFaceBoundingBox(rgbImg)\n",
    "    \n",
    "    if bb1 is None:\n",
    "        raise Exception(\"Unable to find a face: {}\".format(imgPath))\n",
    "    bbs = [bb1]\n",
    "    reps = []\n",
    "    for bb in bbs:\n",
    "        alignedFace = align.align(\n",
    "            96,\n",
    "            rgbImg,\n",
    "            bb,\n",
    "            landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "        if alignedFace is None:\n",
    "            raise Exception(\"Unable to align image: {}\".format(imgPath))\n",
    "\n",
    "        rep = net.forward(alignedFace)\n",
    "\n",
    "        reps.append((bb.center().x, rep))\n",
    "    sreps = sorted(reps, key=lambda x: x[0])\n",
    "    return sreps\n",
    "\n",
    "def infer(model, image):\n",
    "    with open(model, 'rb') as f:\n",
    "        if sys.version_info[0] < 3:\n",
    "                (le, clf) = pickle.load(f)\n",
    "        else:\n",
    "                (le, clf) = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    \n",
    "    print(\"\\n=== {} ===\".format(image))\n",
    "    reps = getRep(image)\n",
    "        \n",
    "    for r in reps:\n",
    "        rep = r[1].reshape(1, -1)\n",
    "        bbx = r[0]\n",
    "        predictions = clf.predict_proba(rep).ravel()\n",
    "        maxI = np.argmax(predictions)\n",
    "        person = le.inverse_transform(maxI)\n",
    "        #confidence = predictions[maxI]\n",
    "        #print(\"Predict {} with {:.2f} confidence.\".format(person.decode('utf-8'), confidence))\n",
    "        return person.decode('utf-8')\n",
    "\n",
    "def run(model, image):\n",
    "    output = infer(model, image)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== test-images/colin_firth.jpg ===\n",
      "colin-firth\n"
     ]
    }
   ],
   "source": [
    "run('generated-embeddings/fullfacescrub/classifier.pkl', 'test-images/colin_firth.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "from PIL import Image\n",
    "import PIL.ImageOps    \n",
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(model, image, name):\n",
    "    output = infer(model, image)\n",
    "    count = 0\n",
    "    n = 0\n",
    "    while(output==name and count<100):\n",
    "        count+=1\n",
    "        #seq = iaa.Sequential([iaa.Invert(n+.05, per_channel=False)])\n",
    "        #temp = cv2.imread(image)\n",
    "        #temp.load()\n",
    "        #t = np.asarray(temp)\n",
    "        #images_aug = seq.augment_images(temp)\n",
    "        # https://github.com/aleju/imgaug/issues/38\n",
    "        #for i, image_aug in enumerate(images_aug):\n",
    "        #    image = 'test-images/' + name + str(count)+'.jpg'    \n",
    "        #    imsave(image, image_aug)\n",
    "        \n",
    "        temp = Image.open(image)\n",
    "        inverted_image = PIL.ImageOps.invert(temp)\n",
    "        image = 'test-images/' + name + '-invert' + str(count)+'.jpg'\n",
    "        inverted_image.save(image)\n",
    "        output=infer(model, image)\n",
    "    print(count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== test-images/colin_firth.jpg ===\n",
      "\n",
      "=== test-images/colin-firth-invert1.jpg ===\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find a face: test-images/colin-firth-invert1.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b3e2f57faa94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generated-embeddings/limited/classifier.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test-images/colin_firth.jpg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'colin-firth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-63c674fa63c2>\u001b[0m in \u001b[0;36minvert\u001b[0;34m(model, image, name)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test-images/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-invert'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0minverted_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5033a77eb1b5>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== {} ===\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5033a77eb1b5>\u001b[0m in \u001b[0;36mgetRep\u001b[0;34m(imgPath)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbb1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to find a face: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mbbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbb1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Unable to find a face: test-images/colin-firth-invert1.jpg"
     ]
    }
   ],
   "source": [
    "invert('generated-embeddings/limited/classifier.pkl', 'test-images/colin_firth.jpg','colin-firth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test-images/colin-firth-invert1.jpg\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== test-images/george_lopez.jpg ===\n",
      "\n",
      "=== test-images/george-lopez-invert1.jpg ===\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "invert('generated-embeddings/limited/classifier.pkl', 'test-images/george_lopez.jpg','george-lopez')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test-images/george-lopez-invert1.jpg\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== test-images/george-lopez-invert1.jpg ===\n",
      "hayden-christensen\n"
     ]
    }
   ],
   "source": [
    "run('generated-embeddings/fullfacescrub/classifier.pkl', 'test-images/george-lopez-invert1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
