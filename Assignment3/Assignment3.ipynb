{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Structured Streaming\n",
    "\n",
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1120707/using-python-to-execute-a-command-on-every-file-in-a-folder\n",
    "#https://stackoverflow.com/questions/7762948/how-to-convert-an-rgb-image-to-numpy-array\n",
    "\n",
    "for filename in os.listdir('lfw'):\n",
    "    name = 'lfw/'+filename\n",
    "    img = Image.open( name )\n",
    "    img.load()\n",
    "    #data = np.asarray(img)\n",
    "    n = 'lfw_np/'+filename[:-4]+'.csv'\n",
    "    #person = np.asarray(filename[:-9])\n",
    "    #data2 = data.tolist().append(person)\n",
    "    #np.savetxt(n, data2,delimiter=',')\n",
    "    #df = pd.DataFrame({'person': [filename[:-9]], 'rgb':[np.asarray(img)]})\n",
    "    df = pd.DataFrame({'person': [filename[:-9]]})\n",
    "    df.to_csv(n, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "\n",
    "# seed for reproducability\n",
    "rand.seed(100)\n",
    "\n",
    "for filename in os.listdir('lfw_np'):\n",
    "    r = randint(1, 2)\n",
    "    current = 'lfw_np/'+filename\n",
    "    if (r==1):\n",
    "        new = 'lfw_batch/'+filename\n",
    "    else:\n",
    "        new = 'lfw_stream/'+filename\n",
    "    # https://stackoverflow.com/questions/8858008/how-to-move-a-file-in-python\n",
    "    os.rename(current, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[person: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From https://docs.databricks.com/_static/notebooks/structured-streaming-python.html\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "inputPath = \"lfw_batch/\"\n",
    "#sc.stop()\n",
    "sc = pyspark.SparkContext(appName=\"stream\")\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Static DataFrame representing data in the csv files\n",
    "staticInputDF = (\n",
    "  sqlContext\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\") \n",
    "    .load(inputPath)\n",
    ")\n",
    "\n",
    "display(staticInputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              person|\n",
      "+--------------------+\n",
      "|Sergei_Alexandrov...|\n",
      "|Sabah_Al-Ahmad_Al...|\n",
      "|Enrique_Haroldo_G...|\n",
      "|Maria_Soledad_Alv...|\n",
      "|Maria_Soledad_Alv...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticInputDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *      # for window() function\n",
    "\n",
    "staticCountsDF = (\n",
    "  staticInputDF\n",
    "    .groupBy(\n",
    "       staticInputDF.person)    \n",
    "    .count()\n",
    ")\n",
    "staticCountsDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsDF.createOrReplaceTempView(\"static_counts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|           person|count|\n",
      "+-----------------+-----+\n",
      "|    George_W_Bush|  253|\n",
      "|     Colin_Powell|  117|\n",
      "|  Donald_Rumsfeld|   64|\n",
      "|       Tony_Blair|   62|\n",
      "|Gerhard_Schroeder|   53|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select person,count from static_counts where count >50 order by count desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From https://docs.databricks.com/_static/notebooks/structured-streaming-python.html\n",
    "inputPath = \"lfw_stream/\"\n",
    "schema = StructType([StructField(\"person\",StringType())])\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = (\n",
    "  sqlContext\n",
    "    .readStream                       \n",
    "    .format(\"csv\")              # Set the schema of the JSON data\n",
    "    .schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .option(\"header\", \"true\") \n",
    "    .load(inputPath)\n",
    ")\n",
    "\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.person)\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"pcs\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6,664 csvs in the lfw_stream folder, so processing the stream will take a long time. First check that it is getting data at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         person|count|\n",
      "+---------------+-----+\n",
      "|Aaron_Patterson|    1|\n",
      "|      Abba_Eban|    1|\n",
      "|  Aaron_Peirsol|    3|\n",
      "|   Aaron_Tippin|    1|\n",
      "|    Aaron_Guiel|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data a few seconds later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              person|count|\n",
      "+--------------------+-----+\n",
      "|     Aaron_Patterson|    1|\n",
      "|           Abba_Eban|    1|\n",
      "|       Aaron_Peirsol|    3|\n",
      "|  Abdel_Madi_Shabneh|    1|\n",
      "|        Aaron_Tippin|    1|\n",
      "|         Aaron_Guiel|    1|\n",
      "| Abdel_Nasser_Assidi|    1|\n",
      "|Abdul_Majeed_Shob...|    1|\n",
      "|        Abdul_Rahman|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again a few seconds after that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              person|count|\n",
      "+--------------------+-----+\n",
      "|     Aaron_Patterson|    1|\n",
      "|           Abba_Eban|    1|\n",
      "|       Aaron_Peirsol|    3|\n",
      "|  Abdel_Madi_Shabneh|    1|\n",
      "|        Aaron_Tippin|    1|\n",
      "|         Aaron_Guiel|    1|\n",
      "| Abdel_Nasser_Assidi|    1|\n",
      "|Abdul_Majeed_Shob...|    1|\n",
      "|        Abdul_Rahman|    1|\n",
      "|   Abdulaziz_Kamilov|    1|\n",
      "|            Abdullah|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now enough data to start querying based on count value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       person|count|\n",
      "+-------------+-----+\n",
      "|Aaron_Peirsol|    3|\n",
      "| Abdullah_Gul|    6|\n",
      "|     Abdullah|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs where count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the counts again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       person|count|\n",
      "+-------------+-----+\n",
      "| Abel_Pacheco|    3|\n",
      "|Aaron_Peirsol|    3|\n",
      "| Abdullah_Gul|    9|\n",
      "|     Abdullah|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs where count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many people have been evaluated so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(person)|\n",
      "+-------------+\n",
      "|           26|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(person) from pcs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After allowing the stream to run for 20 minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(person)|\n",
      "+-------------+\n",
      "|          131|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(person) from pcs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|       221|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select sum(count) from pcs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ~25 minutes of streaming 221 files have been read, representing 131 people. Looking at the query filtering based on count: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              person|count|\n",
      "+--------------------+-----+\n",
      "|        Albert_Costa|    4|\n",
      "|        Adrien_Brody|    7|\n",
      "|       Aaron_Peirsol|    3|\n",
      "|        Alice_Fisher|    2|\n",
      "|           Ali_Naimi|    3|\n",
      "|  Alexander_Losyukov|    3|\n",
      "|           Alex_Sink|    3|\n",
      "|        Alec_Baldwin|    2|\n",
      "|      Akhmed_Zakayev|    2|\n",
      "|    Alexander_Downer|    2|\n",
      "|        Adam_Sandler|    3|\n",
      "|       Ahmed_Chalabi|    2|\n",
      "|         Ai_Sugiyama|    2|\n",
      "|Alvaro_Silva_Cald...|    2|\n",
      "|        Abel_Pacheco|    3|\n",
      "|        Alvaro_Uribe|   21|\n",
      "|        Alvaro_Noboa|    3|\n",
      "|        Ali_Khamenei|    2|\n",
      "|            Abdullah|    3|\n",
      "|        Aldo_Paredes|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs where count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one person has more than 10 pictures in the rows returned, but there are now more than 20 rows meeting the condition and not all the rows are displayed (e.g. Abdulla Gul is not showing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              person|count|\n",
      "+--------------------+-----+\n",
      "|            Abdullah|    3|\n",
      "|Abdullah_Ahmad_Ba...|    1|\n",
      "|        Abdullah_Gul|    9|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from pcs where person like 'Abdullah%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowing this to run for several hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|      6664|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select sum(count) from pcs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(person)|\n",
      "+-------------+\n",
      "|         3499|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(person) from pcs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire \"stream\" data set has been read, so it can be compared to the static one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|           person|count|\n",
      "+-----------------+-----+\n",
      "|    George_W_Bush|  277|\n",
      "|     Colin_Powell|  119|\n",
      "|       Tony_Blair|   82|\n",
      "|  Donald_Rumsfeld|   57|\n",
      "|Gerhard_Schroeder|   56|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select person,count from pcs where count >50 order by count desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the csvs were randomly assigned, the numbers per person differ between the two sets, but the fact that both queries return the same list of names makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C & D Alternate: Deep Learning with Apache Spark and TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First resize images to 32 x 32\n",
    "\n",
    "# Next convert images to numpy arrays to match the CIFAR-10\n",
    "# https://stackoverflow.com/questions/7762948/how-to-convert-an-rgb-image-to-numpy-array\n",
    "\n",
    "# Update labels so that 0 = no face, 1 = face\n",
    "\n",
    "# Move 2100 of the LFW images to a test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
