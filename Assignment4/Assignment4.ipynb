{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "Emily Strong\n",
    "\n",
    "For this assignment I am combining my data set, [FaceScrub](http://vintage.winklerbros.net/facescrub.html), with the [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) to create a human face detection CNN. It is a binary classifier of face/no face. I chose the CIFAR-10 because 6 of the 10 categories are animals which will help ensure that the model specifically detects human faces instead of over generalizing or detecting a particular feature (eg eyes) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "I used convert.py to resize the aligned images generated by the OpenFace pipeline from my project. I then randomly selected 50,000 of them to use in the training set. I added to this 50,000 images from the [CIFAR-10](https://www.kaggle.com/c/cifar-10). Keras comes with CIFAR-10 as one of the built-in data sets however I chose to directly download them instead to ensure they would be in the same format as the face images. Each of these images needs to be converted into an RGB numpy array and labeled as face (1) or no face (0).\n",
    "\n",
    "### 1) Create list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaury-nolasco1600.png'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for filename in os.listdir('train'):\n",
    "    files.append(filename)\n",
    "\n",
    "random.shuffle(files)\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Convert images to numpy arrays and create list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "for filename in files:\n",
    "    name = 'train/'+filename\n",
    "    img = Image.open( name )\n",
    "    img.load()\n",
    "    images.append(np.asarray(img)) \n",
    "    if '-' in filename:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 58,  54,  45],\n",
       "        [ 71,  58,  44],\n",
       "        [ 83,  63,  47],\n",
       "        ..., \n",
       "        [102,  83,  73],\n",
       "        [ 98,  78,  69],\n",
       "        [ 82,  70,  61]],\n",
       "\n",
       "       [[ 55,  50,  43],\n",
       "        [ 72,  62,  49],\n",
       "        [ 82,  65,  50],\n",
       "        ..., \n",
       "        [ 97,  84,  67],\n",
       "        [100,  87,  71],\n",
       "        [ 79,  70,  56]],\n",
       "\n",
       "       [[ 51,  48,  42],\n",
       "        [ 73,  68,  58],\n",
       "        [ 75,  63,  48],\n",
       "        ..., \n",
       "        [ 95,  85,  61],\n",
       "        [ 96,  88,  67],\n",
       "        [ 79,  69,  50]],\n",
       "\n",
       "       ..., \n",
       "       [[ 21,  19,  14],\n",
       "        [ 33,  29,  21],\n",
       "        [ 46,  40,  32],\n",
       "        ..., \n",
       "        [ 60,  48,  33],\n",
       "        [129, 130, 123],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[ 27,  25,  21],\n",
       "        [ 35,  31,  24],\n",
       "        [ 60,  53,  43],\n",
       "        ..., \n",
       "        [ 72,  60,  48],\n",
       "        [ 29,  30,  25],\n",
       "        [166, 167, 162]],\n",
       "\n",
       "       [[ 62,  60,  56],\n",
       "        [ 27,  23,  15],\n",
       "        [ 81,  72,  59],\n",
       "        ..., \n",
       "        [ 64,  50,  41],\n",
       "        [ 26,  26,  26],\n",
       "        [ 37,  38,  35]]], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 32, 32, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.asarray(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Save the image arrays and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wiki.python.org/moin/UsingPickle\n",
    "import pickle\n",
    "pickle.dump( X, open( \"imagearray.pkl\", \"wb\" ) )\n",
    "pickle.dump( Y, open( \"imagearray_labels.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(files, open(\"filenames.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Import pickle files whenever the kernel is restarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X = pickle.load( open( \"imagearray.pkl\", \"rb\" ) )\n",
    "Y = pickle.load(open(\"imagearray_labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Check that image arrays are readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plotData = X[0]\n",
    "plt.imshow(plotData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Deep Learning Model\n",
    "I am creating a convolutional neural network using keras adapted from:\n",
    "* https://github.com/nikbearbrown/NEU_COE/blob/master/CSYE_7245/Week_12/02_Convolutional_Neural_Network.ipynb \n",
    "* https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "batch_size = 32\n",
    "num_classes = 1\n",
    "epochs = 100\n",
    "data_augmentation = False\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models/0')\n",
    "model_name = 'keras_facedetection.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "model.add(Conv2D(32, (5,5), padding='same', input_shape = X_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add second convolution\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add third convolution\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add optimizer\n",
    "opt=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train[0:20000]\n",
    "Y_train_t = Y_train[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 25000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 112s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 112s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 58/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 65/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 66/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 67/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 68/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 69/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 70/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 71/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 72/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 73/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 74/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 75/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 76/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 77/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 78/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 79/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 80/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 81/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 82/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 83/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 84/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 85/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 86/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 87/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 88/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 89/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 90/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 91/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 92/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 93/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 94/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 95/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 96/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 97/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 98/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 99/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 100/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f99357b46d8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_t, Y_train_t, batch_size = batch_size, epochs=epochs, validation_data=(X_test, Y_test), shuffle=True, verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/0/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/0')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 30s 1ms/step\n",
      "Test loss: 7.883190855102539\n",
      "Test accuracy: 0.50552\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2\n",
    "The model failed to train on the data and is performing at chance. I'm going to change the first convolutional layer to use 5x5 convolutions rather than 3x3. I am also increasing the learning rate from .0001 to .001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model2=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "model2.add(Conv2D(32, (5,5), padding='same', input_shape = X_train.shape[1:]))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Add second convolution\n",
    "model2.add(Conv2D(32, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Add pooling\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# Add third convolution\n",
    "model2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(num_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "# Add optimizer\n",
    "opt=keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 25000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 110s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 7.9361 - acc: 0.5022 - val_loss: 7.8832 - val_acc: 0.5055\n",
      "Epoch 15/100\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 7.9345 - acc: 0.5023"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-432ca3ea1f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1250\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.fit(X_train_t, Y_train_t, batch_size = batch_size, epochs=epochs, validation_data=(X_test, Y_test), shuffle=True, verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3\n",
    "The model is again performing at chance. I am adding in an image data generator which normalizes and creates modified versions of the images to increase the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# augmentation configuration for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# augmentation configuration for testing:\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size) \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model3=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "model3.add(Conv2D(32, (5,5), padding='same', input_shape = X_train.shape[1:]))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Add second convolution\n",
    "model3.add(Conv2D(32, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Add pooling\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Add third convolution\n",
    "model3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(512))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(num_classes))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "# Add optimizer\n",
    "opt=keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model3.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 167s 83ms/step - loss: 7.9433 - acc: 0.5018 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 167s 83ms/step - loss: 7.9876 - acc: 0.4990 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 167s 83ms/step - loss: 8.0429 - acc: 0.4955 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 8.0061 - acc: 0.4978 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9951 - acc: 0.4985 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9981 - acc: 0.4983 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9866 - acc: 0.4990 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9702 - acc: 0.5001 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 167s 83ms/step - loss: 8.0384 - acc: 0.4958 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 167s 83ms/step - loss: 8.0066 - acc: 0.4978 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 167s 83ms/step - loss: 8.0086 - acc: 0.4977 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 7.9662 - acc: 0.5003 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9747 - acc: 0.4998 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 8.0096 - acc: 0.4976 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 7.9911 - acc: 0.4987 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 8.0001 - acc: 0.4982 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9617 - acc: 0.5006 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9976 - acc: 0.4983 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 8.0280 - acc: 0.4964 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 7.9169 - acc: 0.5034 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 8.0738 - acc: 0.4936 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 8.0011 - acc: 0.4981 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 168s 84ms/step - loss: 7.9956 - acc: 0.4985 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 167s 84ms/step - loss: 7.9672 - acc: 0.5002 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 25/50\n",
      " 445/2000 [=====>........................] - ETA: 1:57 - loss: 7.9264 - acc: 0.5028"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-42b51b8dad20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         verbose=1, callbacks=[tensorboard])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 4\n",
    "The model is still performing at chance. I am changing the optimizer from rmsprop to SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model4=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "model4.add(Conv2D(32, (5,5), padding='same', input_shape = X_train.shape[1:]))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Add second convolution\n",
    "model4.add(Conv2D(32, (3, 3)))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Add pooling\n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "# Add third convolution\n",
    "model4.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "model4.add(Conv2D(64, (3, 3)))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(512))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(num_classes))\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "# Add optimizer\n",
    "opt2=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "# Compile model\n",
    "model4.compile(loss='binary_crossentropy', optimizer=opt2, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(\n",
    "        X_train_t, Y_train_t, \n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9727 - acc: 0.4999 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9254 - acc: 0.5029 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9109 - acc: 0.5038 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9428 - acc: 0.5018 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9288 - acc: 0.5027 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9313 - acc: 0.5025 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 162s 81ms/step - loss: 7.9528 - acc: 0.5012 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 164s 82ms/step - loss: 7.9214 - acc: 0.5031 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 287s 143ms/step - loss: 7.9134 - acc: 0.5036 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 302s 151ms/step - loss: 7.9617 - acc: 0.5006 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 303s 152ms/step - loss: 7.9368 - acc: 0.5022 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 303s 152ms/step - loss: 7.9662 - acc: 0.5003 - val_loss: 7.8753 - val_acc: 0.5060\n",
      "Epoch 13/50\n",
      "1661/2000 [=======================>......] - ETA: 45s - loss: 7.9016 - acc: 0.5044"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2fa4f0c8f71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         verbose=1, callbacks=[tensorboard])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 5\n",
    "I adapted the initial network architecture from a tutorial meant for a 10 category classifier. I am changing it to match a [binary classifier tutorial](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721) that similar to my model uses the CIFAR-10 for the \"not\" category. Specifically, I am removing the dropouts from the convolution layers and removing one convolution layer. I am also changing the optimizer to Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Set the parameters\n",
    "batch_size = 32\n",
    "num_classes = 1\n",
    "epochs = 100\n",
    "data_augmentation = False\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models/0')\n",
    "model_name = 'keras_facedetection.h5'\n",
    "\n",
    "# Initialize model\n",
    "model5=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "model5.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:]))\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "# Add second convolution\n",
    "#model3.add(Conv2D(32, (3, 3)))\n",
    "#model3.add(Activation('relu'))\n",
    "\n",
    "# Add pooling\n",
    "model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model3.add(Dropout(0.25))\n",
    "\n",
    "# Add third convolution\n",
    "model5.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "model5.add(Conv2D(64, (3, 3)))\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "model5.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "#model3.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(512))\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes))\n",
    "model5.add(Activation('softmax'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "model5.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train[0:20000]\n",
    "Y_train_t = Y_train[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# augmentation configuration for training:\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# augmentation configuration for testing:\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32000/32000 [==============================] - 325s 10ms/step - loss: 7.9607 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 2/50\n",
      "32000/32000 [==============================] - 324s 10ms/step - loss: 7.9593 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 3/50\n",
      "32000/32000 [==============================] - 324s 10ms/step - loss: 7.9587 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 4/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9559 - acc: 0.5010 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 5/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9610 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 6/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9606 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 7/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9562 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 8/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9598 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 9/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9627 - acc: 0.5005 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 10/50\n",
      "32000/32000 [==============================] - 320s 10ms/step - loss: 7.9598 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 11/50\n",
      "32000/32000 [==============================] - 320s 10ms/step - loss: 7.9563 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 12/50\n",
      "32000/32000 [==============================] - 320s 10ms/step - loss: 7.9616 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 13/50\n",
      "32000/32000 [==============================] - 320s 10ms/step - loss: 7.9588 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 14/50\n",
      "32000/32000 [==============================] - 320s 10ms/step - loss: 7.9568 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 15/50\n",
      "32000/32000 [==============================] - 320s 10ms/step - loss: 7.9584 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 16/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9612 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 17/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9594 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 18/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9581 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 19/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9620 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 20/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 21/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9560 - acc: 0.5010 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 22/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9620 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 23/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9589 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 24/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9631 - acc: 0.5005 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 25/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9557 - acc: 0.5010 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 26/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9586 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 27/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9618 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 28/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9563 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 29/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9609 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 30/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9578 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 31/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9597 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 32/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9611 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 33/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9601 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 34/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9567 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 35/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9623 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 36/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9516 - acc: 0.5012 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 37/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9653 - acc: 0.5004 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 38/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9559 - acc: 0.5010 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 39/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9612 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 40/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9597 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 41/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9574 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 42/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9618 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 43/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9612 - acc: 0.5006 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 44/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9583 - acc: 0.5008 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 45/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9567 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 46/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9603 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 47/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9571 - acc: 0.5009 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 48/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9606 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 49/50\n",
      "32000/32000 [==============================] - 318s 10ms/step - loss: 7.9605 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n",
      "Epoch 50/50\n",
      "32000/32000 [==============================] - 319s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 8.0920 - val_acc: 0.4924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44500862e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000*batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/5/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/5')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model5.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 111us/step\n",
      "Test loss: 8.006903791809082\n",
      "Test accuracy: 0.49776\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model5.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 6\n",
    "Since none of my previous attempts to train the network have worked, I am adding initializers, as the Tensorboard tutorial shown in class demonstrated that improper initialization can cause the behavior I am seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "modelG1=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelG1.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:], \n",
    "                   kernel_initializer='random_uniform', \n",
    "                   bias_initializer='ones'))\n",
    "modelG1.add(Activation('relu'))\n",
    "\n",
    "# Add second convolution\n",
    "#model3.add(Conv2D(32, (3, 3)))\n",
    "#model3.add(Activation('relu'))\n",
    "\n",
    "# Add pooling\n",
    "modelG1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model3.add(Dropout(0.25))\n",
    "\n",
    "# Add third convolution\n",
    "modelG1.add(Conv2D(64, (3, 3), padding='same', \n",
    "                   kernel_initializer='random_uniform', \n",
    "                   bias_initializer='ones'))\n",
    "modelG1.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelG1.add(Conv2D(64, (3, 3), \n",
    "                   kernel_initializer='random_uniform', \n",
    "                   bias_initializer='ones'))\n",
    "modelG1.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "modelG1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "#model3.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "modelG1.add(Flatten())\n",
    "modelG1.add(Dense(512, \n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG1.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelG1.add(Dropout(0.5))\n",
    "modelG1.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG1.add(Activation('softmax'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelG1.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size) \n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32000/32000 [==============================] - 328s 10ms/step - loss: 7.9575 - acc: 0.5009 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 2/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9602 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 3/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9598 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 4/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9599 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 5/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9569 - acc: 0.5009 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 6/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9599 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 7/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9606 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 8/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9550 - acc: 0.5010 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 9/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9650 - acc: 0.5004 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 10/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9599 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 11/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9578 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 12/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9589 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 13/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9582 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 14/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9601 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 15/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9621 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 16/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9557 - acc: 0.5010 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 17/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9610 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 18/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9563 - acc: 0.5009 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 19/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9616 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 20/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9620 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 21/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9528 - acc: 0.5012 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 22/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9632 - acc: 0.5005 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 23/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 24/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 25/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9597 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 26/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9577 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 27/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9604 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 28/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9572 - acc: 0.5009 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 29/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9623 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 30/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9583 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 31/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 32/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9577 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 33/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9577 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 34/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9598 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 35/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9598 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 36/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9617 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 37/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9534 - acc: 0.5011 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 38/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9611 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 39/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9605 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 40/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9605 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 41/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9583 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 42/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9623 - acc: 0.5006 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 43/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9570 - acc: 0.5009 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 44/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9566 - acc: 0.5009 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 45/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9630 - acc: 0.5005 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 46/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9586 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 47/50\n",
      "32000/32000 [==============================] - 321s 10ms/step - loss: 7.9601 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 48/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9592 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 49/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9605 - acc: 0.5007 - val_loss: 8.0497 - val_acc: 0.4951\n",
      "Epoch 50/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9586 - acc: 0.5008 - val_loss: 8.0497 - val_acc: 0.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f444b73e630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelG1.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000*batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "    \n",
    "    \n",
    "    \n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/G1/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/G1')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "modelG1.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 110us/step\n",
      "Test loss: 8.006903791809082\n",
      "Test accuracy: 0.49776\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scoresG1 = modelG1.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scoresG1[0])\n",
    "print('Test accuracy:', scoresG1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 7\n",
    "I am trying random normally distributed intializers for both kernel and bias.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "modelG2=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelG2.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:], \n",
    "                   kernel_initializer='random_normal', \n",
    "                   bias_initializer='random_normal'))\n",
    "modelG2.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# Add pooling\n",
    "modelG2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelG2.add(Conv2D(64, (3, 3), padding='same', \n",
    "                   kernel_initializer='random_normal', \n",
    "                   bias_initializer='random_normal'))\n",
    "modelG2.add(Activation('relu'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelG2.add(Conv2D(64, (3, 3), \n",
    "                   kernel_initializer='random_normal', \n",
    "                   bias_initializer='random_normal'))\n",
    "modelG2.add(Activation('relu'))\n",
    "\n",
    "# Add second pooling\n",
    "modelG2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add dense layer\n",
    "modelG2.add(Flatten())\n",
    "modelG2.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelG2.add(Activation('relu'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelG2.add(Dropout(0.5))\n",
    "modelG2.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelG2.add(Activation('softmax'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelG2.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32000/32000 [==============================] - 329s 10ms/step - loss: 7.9603 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "32000/32000 [==============================] - 327s 10ms/step - loss: 7.9598 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "32000/32000 [==============================] - 324s 10ms/step - loss: 7.9586 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9593 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9575 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9607 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 8/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 9/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9617 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 10/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9536 - acc: 0.5011 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 11/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9616 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9600 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 13/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9604 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 14/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9612 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 15/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9553 - acc: 0.5010 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 16/50\n",
      "32000/32000 [==============================] - 324s 10ms/step - loss: 7.9586 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 17/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9606 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 18/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9580 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 19/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9566 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 20/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9636 - acc: 0.5005 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 21/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9624 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 22/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9564 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 23/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9585 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 24/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9609 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 25/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9594 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 26/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9568 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9599 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 28/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9587 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 29/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 30/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9578 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 31/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9637 - acc: 0.5005 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 32/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9550 - acc: 0.5010 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 33/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9618 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 34/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9595 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 35/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9587 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 36/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9628 - acc: 0.5005 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 37/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9567 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 38/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9586 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 39/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9571 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 40/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9624 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 41/50\n",
      "32000/32000 [==============================] - 323s 10ms/step - loss: 7.9584 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 42/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9601 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9581 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 44/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9620 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9561 - acc: 0.5009 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 46/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9608 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 47/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9590 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 48/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9591 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 49/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9600 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 50/50\n",
      "32000/32000 [==============================] - 322s 10ms/step - loss: 7.9597 - acc: 0.5007 - val_loss: 7.9712 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44500869b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelG2.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000*batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/G2/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/G2')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "modelG2.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 116us/step\n",
      "Test loss: 8.006903791809082\n",
      "Test accuracy: 0.49776\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scoresG2 = modelG2.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scoresG2[0])\n",
    "print('Test accuracy:', scoresG2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I haven't yet gotten the network to successfully train/stop performing at chance, I will move on and see if any of the hyperparameters might be the problem.\n",
    "\n",
    "# Part B: Activation Function\n",
    "## Attempt B1\n",
    "I haven't had success with relu, so I am comparing it to the sigmoid and hard_sigmoid activations. I am also increasing the transformations the image data generator performs to increase the diversity of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train_t = X_train[0:20000]\n",
    "Y_train_t = Y_train[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/b1_sigmoid\")\n",
    "\n",
    "# Initialize model\n",
    "modelB1=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelB1.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB1.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "# Add pooling\n",
    "modelB1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelB1.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelB1.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelB1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add dense layer\n",
    "modelB1.add(Flatten())\n",
    "modelB1.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelB1.add(Dropout(0.5))\n",
    "modelB1.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelB1.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.0743 - acc: 0.9648 - val_loss: 0.0239 - val_acc: 0.9917\n",
      "Epoch 2/25\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0082 - val_acc: 0.9976\n",
      "Epoch 3/25\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0071 - val_acc: 0.9983\n",
      "Epoch 4/25\n",
      "16000/16000 [==============================] - 166s 10ms/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0045 - val_acc: 0.9994\n",
      "Epoch 5/25\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.0078 - acc: 0.9977 - val_loss: 0.0049 - val_acc: 0.9991\n",
      "Epoch 6/25\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0046 - val_acc: 0.9991\n",
      "Epoch 7/25\n",
      "16000/16000 [==============================] - 165s 10ms/step - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0055 - val_acc: 0.9988\n",
      "Epoch 8/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 9/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0042 - val_acc: 0.9991\n",
      "Epoch 10/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0048 - val_acc: 0.9991\n",
      "Epoch 11/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0040 - val_acc: 0.9992\n",
      "Epoch 12/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0034 - val_acc: 0.9994\n",
      "Epoch 13/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0042 - val_acc: 0.9993\n",
      "Epoch 14/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0041 - val_acc: 0.9996\n",
      "Epoch 15/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0048 - val_acc: 0.9990\n",
      "Epoch 16/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0071 - val_acc: 0.9984\n",
      "Epoch 17/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0059 - val_acc: 0.9990\n",
      "Epoch 18/25\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0053 - val_acc: 0.9989\n",
      "Epoch 19/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0092 - val_acc: 0.9984\n",
      "Epoch 20/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0041 - val_acc: 0.9992\n",
      "Epoch 21/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0077 - val_acc: 0.9991\n",
      "Epoch 22/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0052 - val_acc: 0.9993\n",
      "Epoch 23/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0053 - val_acc: 0.9993\n",
      "Epoch 24/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0069 - val_acc: 0.9991\n",
      "Epoch 25/25\n",
      "16000/16000 [==============================] - 163s 10ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0056 - val_acc: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2406b05748>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB1.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1000*batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/B1/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/B1')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "modelB1.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 122us/step\n",
      "Test loss: 0.027202206631661393\n",
      "Test accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scoresB1 = modelB1.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scoresB1[0])\n",
    "print('Test accuracy:', scoresB1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/b2_hard_sigmoid\")\n",
    "\n",
    "# Initialize model\n",
    "modelB2=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelB2.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:], \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB2.add(Activation('hard_sigmoid'))\n",
    "\n",
    "\n",
    "# Add pooling\n",
    "modelB2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelB2.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB2.add(Activation('hard_sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelB2.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB2.add(Activation('hard_sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelB2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add dense layer\n",
    "modelB2.add(Flatten())\n",
    "modelB2.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB2.add(Activation('hard_sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelB2.add(Dropout(0.5))\n",
    "modelB2.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelB2.add(Activation('hard_sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelB2.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.7042 - acc: 0.5001 - val_loss: 0.6935 - val_acc: 0.5009\n",
      "Epoch 2/25\n",
      "16000/16000 [==============================] - 209s 13ms/step - loss: 0.7000 - acc: 0.4990 - val_loss: 0.6946 - val_acc: 0.4991\n",
      "Epoch 3/25\n",
      "16000/16000 [==============================] - 206s 13ms/step - loss: 0.6989 - acc: 0.4985 - val_loss: 0.6935 - val_acc: 0.4991\n",
      "Epoch 4/25\n",
      "16000/16000 [==============================] - 205s 13ms/step - loss: 0.6976 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.5009\n",
      "Epoch 5/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.6964 - acc: 0.4998 - val_loss: 0.6956 - val_acc: 0.4991\n",
      "Epoch 6/25\n",
      "16000/16000 [==============================] - 206s 13ms/step - loss: 0.6955 - acc: 0.5028 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 7/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.6949 - acc: 0.4994 - val_loss: 0.6944 - val_acc: 0.5009\n",
      "Epoch 8/25\n",
      "16000/16000 [==============================] - 206s 13ms/step - loss: 0.6942 - acc: 0.5003 - val_loss: 0.6935 - val_acc: 0.4991\n",
      "Epoch 9/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.6937 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5009\n",
      "Epoch 10/25\n",
      "16000/16000 [==============================] - 206s 13ms/step - loss: 0.6934 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 11/25\n",
      "16000/16000 [==============================] - 206s 13ms/step - loss: 0.6933 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.5009\n",
      "Epoch 12/25\n",
      "16000/16000 [==============================] - 204s 13ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5009\n",
      "Epoch 13/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 14/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 15/25\n",
      "16000/16000 [==============================] - 204s 13ms/step - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 16/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5009\n",
      "Epoch 17/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 18/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.5009\n",
      "Epoch 19/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 20/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5009\n",
      "Epoch 21/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 22/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 23/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 24/25\n",
      "16000/16000 [==============================] - 203s 13ms/step - loss: 0.6932 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4991\n",
      "Epoch 25/25\n",
      "16000/16000 [==============================] - 207s 13ms/step - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f240697f4a8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelB2.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=1000*batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/B2/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/B2')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "modelB2.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 132us/step\n",
      "Test loss: 0.6931535883712768\n",
      "Test accuracy: 0.49944\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scoresB2 = modelB2.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scoresB2[0])\n",
    "print('Test accuracy:', scoresB2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Cost Function\n",
    "I am concerned about the model overfitting so I am adding more transformations to the image data generator and decreasing the number of images used in each epoch. I am testing mean square error, and because of the changes I am rerunning binary cross entropy to ensure they can be compared.\n",
    "\n",
    "## Attempt C1 - Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/c/mse\")\n",
    "\n",
    "# Initialize model\n",
    "modelC1=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelC1.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC1.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "# Add pooling\n",
    "modelC1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelC1.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelC1.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelC1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add dense layer\n",
    "modelC1.add(Flatten())\n",
    "modelC1.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelC1.add(Dropout(0.5))\n",
    "modelC1.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelC1.compile(loss='mean_squared_error', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 84s 10ms/step - loss: 0.0870 - acc: 0.8717 - val_loss: 0.0348 - val_acc: 0.9552\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0425 - acc: 0.9449 - val_loss: 0.0268 - val_acc: 0.9680\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0361 - acc: 0.9527 - val_loss: 0.0235 - val_acc: 0.9706\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0338 - acc: 0.9554 - val_loss: 0.0191 - val_acc: 0.9769\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0307 - acc: 0.9604 - val_loss: 0.0188 - val_acc: 0.9761\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0284 - acc: 0.9634 - val_loss: 0.0148 - val_acc: 0.9823\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0270 - acc: 0.9652 - val_loss: 0.0166 - val_acc: 0.9784\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0260 - acc: 0.9667 - val_loss: 0.0152 - val_acc: 0.9811\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0242 - acc: 0.9695 - val_loss: 0.0126 - val_acc: 0.9841\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0227 - acc: 0.9712 - val_loss: 0.0126 - val_acc: 0.9850\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0216 - acc: 0.9723 - val_loss: 0.0173 - val_acc: 0.9774\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0205 - acc: 0.9737 - val_loss: 0.0099 - val_acc: 0.9877\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0197 - acc: 0.9752 - val_loss: 0.0126 - val_acc: 0.9838\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0189 - acc: 0.9763 - val_loss: 0.0098 - val_acc: 0.9880\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0181 - acc: 0.9774 - val_loss: 0.0081 - val_acc: 0.9895\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0172 - acc: 0.9783 - val_loss: 0.0090 - val_acc: 0.9883\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0168 - acc: 0.9790 - val_loss: 0.0105 - val_acc: 0.9866\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0161 - acc: 0.9798 - val_loss: 0.0087 - val_acc: 0.9893\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0151 - acc: 0.9812 - val_loss: 0.0083 - val_acc: 0.9895\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0159 - acc: 0.9803 - val_loss: 0.0095 - val_acc: 0.9883\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0148 - acc: 0.9816 - val_loss: 0.0083 - val_acc: 0.9889\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0139 - acc: 0.9827 - val_loss: 0.0065 - val_acc: 0.9924\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0134 - acc: 0.9832 - val_loss: 0.0104 - val_acc: 0.9883\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0142 - acc: 0.9826 - val_loss: 0.0056 - val_acc: 0.9923\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0130 - acc: 0.9841 - val_loss: 0.0065 - val_acc: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23d655a978>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelC1.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500*batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/C1/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/C1')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "modelC1.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 119us/step\n",
      "Test loss: 0.10283870698129707\n",
      "Test accuracy: 0.88524\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scoresC1 = modelC1.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scoresC1[0])\n",
    "print('Test accuracy:', scoresC1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt C2 - Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/c/binary_crossentropy\")\n",
    "\n",
    "# Initialize model\n",
    "modelC2=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelC2.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC2.add(Activation('sigmoid'))\n",
    "\n",
    "# Add pooling\n",
    "modelC2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelC2.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC2.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelC2.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC2.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelC2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add dense layer\n",
    "modelC2.add(Flatten())\n",
    "modelC2.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC2.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelC2.add(Dropout(0.5))\n",
    "modelC2.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelC2.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelC2.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.2629 - acc: 0.8802 - val_loss: 0.0980 - val_acc: 0.9627\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.1344 - acc: 0.9484 - val_loss: 0.0755 - val_acc: 0.9746\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.1201 - acc: 0.9545 - val_loss: 0.0672 - val_acc: 0.9777\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.1108 - acc: 0.9584 - val_loss: 0.0575 - val_acc: 0.9790\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0961 - acc: 0.9645 - val_loss: 0.0546 - val_acc: 0.9817\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0866 - acc: 0.9683 - val_loss: 0.0449 - val_acc: 0.9836\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0786 - acc: 0.9717 - val_loss: 0.0451 - val_acc: 0.9850\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0747 - acc: 0.9739 - val_loss: 0.0496 - val_acc: 0.9828\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0710 - acc: 0.9747 - val_loss: 0.0337 - val_acc: 0.9884\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0664 - acc: 0.9761 - val_loss: 0.0316 - val_acc: 0.9903\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0633 - acc: 0.9779 - val_loss: 0.0295 - val_acc: 0.9905\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0612 - acc: 0.9780 - val_loss: 0.0282 - val_acc: 0.9903\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0570 - acc: 0.9802 - val_loss: 0.0344 - val_acc: 0.9881\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0560 - acc: 0.9803 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0513 - acc: 0.9820 - val_loss: 0.0260 - val_acc: 0.9920\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0514 - acc: 0.9819 - val_loss: 0.0212 - val_acc: 0.9938\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0479 - acc: 0.9835 - val_loss: 0.0168 - val_acc: 0.9953\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0467 - acc: 0.9839 - val_loss: 0.0235 - val_acc: 0.9930\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0451 - acc: 0.9845 - val_loss: 0.0202 - val_acc: 0.9941\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0440 - acc: 0.9849 - val_loss: 0.0239 - val_acc: 0.9927\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0426 - acc: 0.9851 - val_loss: 0.0155 - val_acc: 0.9952\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0405 - acc: 0.9862 - val_loss: 0.0156 - val_acc: 0.9953\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0391 - acc: 0.9869 - val_loss: 0.0215 - val_acc: 0.9930\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0375 - acc: 0.9876 - val_loss: 0.0194 - val_acc: 0.9939\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 84s 11ms/step - loss: 0.0362 - acc: 0.9877 - val_loss: 0.0241 - val_acc: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23d4f00ac8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelC2.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500*batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ubuntu/notebooks/saved_models/C2/keras_facedetection.h5 \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/C2')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "modelC2.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 122us/step\n",
      "Test loss: 0.9210436370277405\n",
      "Test accuracy: 0.83892\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scoresC2 = modelC2.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scoresC2[0])\n",
    "print('Test accuracy:', scoresC2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Epochs\n",
    "I have selected binary cross entropy based on the results in C. Here I am comparing the 25 epochs from that run to 35 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.3943 - acc: 0.7804 - val_loss: 0.1380 - val_acc: 0.9514\n",
      "Epoch 2/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.1572 - acc: 0.9406 - val_loss: 0.0910 - val_acc: 0.9708\n",
      "Epoch 3/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.1270 - acc: 0.9516 - val_loss: 0.0774 - val_acc: 0.9673\n",
      "Epoch 4/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.1118 - acc: 0.9575 - val_loss: 0.0835 - val_acc: 0.9705\n",
      "Epoch 5/35\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.0986 - acc: 0.9633 - val_loss: 0.0528 - val_acc: 0.9819\n",
      "Epoch 6/35\n",
      "8000/8000 [==============================] - 88s 11ms/step - loss: 0.0885 - acc: 0.9673 - val_loss: 0.0523 - val_acc: 0.9835\n",
      "Epoch 7/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0828 - acc: 0.9702 - val_loss: 0.0520 - val_acc: 0.9816\n",
      "Epoch 8/35\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.0773 - acc: 0.9725 - val_loss: 0.0414 - val_acc: 0.9858\n",
      "Epoch 9/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0724 - acc: 0.9738 - val_loss: 0.0650 - val_acc: 0.9775\n",
      "Epoch 10/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0667 - acc: 0.9757 - val_loss: 0.0307 - val_acc: 0.9896\n",
      "Epoch 11/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0654 - acc: 0.9769 - val_loss: 0.0318 - val_acc: 0.9897\n",
      "Epoch 12/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0629 - acc: 0.9779 - val_loss: 0.0265 - val_acc: 0.9915\n",
      "Epoch 13/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0589 - acc: 0.9792 - val_loss: 0.0461 - val_acc: 0.9852\n",
      "Epoch 14/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0574 - acc: 0.9796 - val_loss: 0.0357 - val_acc: 0.9891\n",
      "Epoch 15/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0541 - acc: 0.9806 - val_loss: 0.0257 - val_acc: 0.9918\n",
      "Epoch 16/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0516 - acc: 0.9817 - val_loss: 0.0260 - val_acc: 0.9920\n",
      "Epoch 17/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0497 - acc: 0.9825 - val_loss: 0.0327 - val_acc: 0.9891\n",
      "Epoch 18/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0486 - acc: 0.9831 - val_loss: 0.0313 - val_acc: 0.9898\n",
      "Epoch 19/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0462 - acc: 0.9840 - val_loss: 0.0398 - val_acc: 0.9864\n",
      "Epoch 20/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0438 - acc: 0.9849 - val_loss: 0.0292 - val_acc: 0.9905\n",
      "Epoch 21/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0428 - acc: 0.9850 - val_loss: 0.0209 - val_acc: 0.9937\n",
      "Epoch 22/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0418 - acc: 0.9860 - val_loss: 0.0207 - val_acc: 0.9942\n",
      "Epoch 23/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0405 - acc: 0.9864 - val_loss: 0.0160 - val_acc: 0.9949\n",
      "Epoch 24/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0384 - acc: 0.9871 - val_loss: 0.0194 - val_acc: 0.9944\n",
      "Epoch 25/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0379 - acc: 0.9868 - val_loss: 0.0259 - val_acc: 0.9923\n",
      "Epoch 26/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0359 - acc: 0.9880 - val_loss: 0.0188 - val_acc: 0.9948\n",
      "Epoch 27/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0363 - acc: 0.9876 - val_loss: 0.0151 - val_acc: 0.9960\n",
      "Epoch 28/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0346 - acc: 0.9882 - val_loss: 0.0143 - val_acc: 0.9958\n",
      "Epoch 29/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0334 - acc: 0.9888 - val_loss: 0.0144 - val_acc: 0.9962\n",
      "Epoch 30/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0330 - acc: 0.9891 - val_loss: 0.0237 - val_acc: 0.9930\n",
      "Epoch 31/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0325 - acc: 0.9892 - val_loss: 0.0223 - val_acc: 0.9936\n",
      "Epoch 32/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0315 - acc: 0.9894 - val_loss: 0.0148 - val_acc: 0.9954\n",
      "Epoch 33/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0302 - acc: 0.9900 - val_loss: 0.0199 - val_acc: 0.9943\n",
      "Epoch 34/35\n",
      "8000/8000 [==============================] - 85s 11ms/step - loss: 0.0299 - acc: 0.9901 - val_loss: 0.0147 - val_acc: 0.9959\n",
      "Epoch 35/35\n",
      "8000/8000 [==============================] - 86s 11ms/step - loss: 0.0288 - acc: 0.9903 - val_loss: 0.0174 - val_acc: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24067a4fd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/d/35_epochs\")\n",
    "\n",
    "# Initialize model\n",
    "modelD1=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelD1.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelD1.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# Add pooling\n",
    "modelD1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelD1.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelD1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelD1.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelD1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelD1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "#model3.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "modelD1.add(Flatten())\n",
    "modelD1.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelD1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelD1.add(Dropout(0.5))\n",
    "modelD1.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelD1.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelD1.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "modelD1.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500*batch_size,\n",
    "        epochs=35,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E: Gradient Estimation\n",
    "I have been using the Adam optimizer. Here I test SGD and will compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.7042 - acc: 0.5011 - val_loss: 0.6936 - val_acc: 0.5029\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6968 - acc: 0.4991 - val_loss: 0.6946 - val_acc: 0.4971\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6947 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5029\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6940 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.4971\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6936 - acc: 0.5033 - val_loss: 0.6929 - val_acc: 0.5029\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6934 - acc: 0.5035 - val_loss: 0.6929 - val_acc: 0.5029\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6930 - acc: 0.5082 - val_loss: 0.6917 - val_acc: 0.5029\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.6835 - acc: 0.5479 - val_loss: 0.5643 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.3361 - acc: 0.8703 - val_loss: 0.2712 - val_acc: 0.9085\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.2669 - acc: 0.9050 - val_loss: 0.2258 - val_acc: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23d4967cf8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/e/sgd\")\n",
    "\n",
    "# Initialize model\n",
    "modelE=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelE.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelE.add(Activation('sigmoid'))\n",
    "\n",
    "# Add pooling\n",
    "modelE.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelE.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelE.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelE.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelE.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelE.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "#model3.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "modelE.add(Flatten())\n",
    "modelE.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelE.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelE.add(Dropout(0.5))\n",
    "modelE.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelE.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "# Compile model\n",
    "modelE.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "modelE.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500*batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F: Network Architecture\n",
    "The initial model I attempted to replicate had one more convolutional layer than I have been using, and additional dropouts to regularize the network. Now that I have a working model, I am going to compare the architecture I have been using to this alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 105s 13ms/step - loss: 0.2569 - acc: 0.8910 - val_loss: 0.1288 - val_acc: 0.9495\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 0.1520 - acc: 0.9426 - val_loss: 0.0965 - val_acc: 0.9660\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 0.1225 - acc: 0.9534 - val_loss: 0.0882 - val_acc: 0.9712\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.1096 - acc: 0.9594 - val_loss: 0.0577 - val_acc: 0.9798\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0972 - acc: 0.9643 - val_loss: 0.0667 - val_acc: 0.9763\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 0.0889 - acc: 0.9674 - val_loss: 0.0539 - val_acc: 0.9807\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0833 - acc: 0.9699 - val_loss: 0.0608 - val_acc: 0.9777\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 0.0792 - acc: 0.9717 - val_loss: 0.0414 - val_acc: 0.9858\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0730 - acc: 0.9737 - val_loss: 0.0400 - val_acc: 0.9849\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0697 - acc: 0.9749 - val_loss: 0.0317 - val_acc: 0.9885\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 107s 13ms/step - loss: 0.0659 - acc: 0.9767 - val_loss: 0.0354 - val_acc: 0.9871\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 0.0624 - acc: 0.9778 - val_loss: 0.0264 - val_acc: 0.9909\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 107s 13ms/step - loss: 0.0601 - acc: 0.9791 - val_loss: 0.0252 - val_acc: 0.9921\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0561 - acc: 0.9802 - val_loss: 0.0267 - val_acc: 0.9906\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 0.0549 - acc: 0.9811 - val_loss: 0.0227 - val_acc: 0.9922\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 0.0517 - acc: 0.9821 - val_loss: 0.0234 - val_acc: 0.9925\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 107s 13ms/step - loss: 0.0496 - acc: 0.9829 - val_loss: 0.0189 - val_acc: 0.9948\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 108s 14ms/step - loss: 0.0471 - acc: 0.9836 - val_loss: 0.0196 - val_acc: 0.9945\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 0.0461 - acc: 0.9840 - val_loss: 0.0173 - val_acc: 0.9945\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 107s 13ms/step - loss: 0.0458 - acc: 0.9839 - val_loss: 0.0193 - val_acc: 0.9941\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0425 - acc: 0.9857 - val_loss: 0.0169 - val_acc: 0.9951\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0442 - acc: 0.9852 - val_loss: 0.0171 - val_acc: 0.9943\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 107s 13ms/step - loss: 0.0412 - acc: 0.9860 - val_loss: 0.0176 - val_acc: 0.9943\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 109s 14ms/step - loss: 0.0394 - acc: 0.9869 - val_loss: 0.0154 - val_acc: 0.9951\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 108s 13ms/step - loss: 0.0384 - acc: 0.9869 - val_loss: 0.0205 - val_acc: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23b3c6ecc0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/f/additional_layer_dropout\")\n",
    "\n",
    "# Initialize model\n",
    "modelF=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelF.add(Conv2D(32, (5,5), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelF.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second convolution\n",
    "modelF.add(Conv2D(32, (3, 3),\n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelF.add(Activation('sigmoid'))\n",
    "\n",
    "# Add pooling\n",
    "modelF.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "modelF.add(Dropout(0.25))\n",
    "\n",
    "# Add third convolution\n",
    "modelF.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelF.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelF.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelF.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelF.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add dropout layer to regularized network \n",
    "modelF.add(Dropout(0.25))\n",
    "\n",
    "# Add dense layer\n",
    "modelF.add(Flatten())\n",
    "modelF.add(Dense(512, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelF.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelF.add(Dropout(0.5))\n",
    "modelF.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_normal', \n",
    "                  bias_initializer='random_normal'))\n",
    "modelF.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelF.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "modelF.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500*batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G: Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.5042 - acc: 0.6929 - val_loss: 0.1773 - val_acc: 0.9406\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1977 - acc: 0.9306 - val_loss: 0.1305 - val_acc: 0.9513\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1708 - acc: 0.9364 - val_loss: 0.1196 - val_acc: 0.9470\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1520 - acc: 0.9414 - val_loss: 0.0856 - val_acc: 0.9688\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1340 - acc: 0.9489 - val_loss: 0.0932 - val_acc: 0.9661\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1245 - acc: 0.9534 - val_loss: 0.0851 - val_acc: 0.9630\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1176 - acc: 0.9569 - val_loss: 0.0739 - val_acc: 0.9679\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1130 - acc: 0.9578 - val_loss: 0.0628 - val_acc: 0.9773\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1081 - acc: 0.9606 - val_loss: 0.0664 - val_acc: 0.9766\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 87s 11ms/step - loss: 0.1048 - acc: 0.9617 - val_loss: 0.0926 - val_acc: 0.9663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f23b3d94ac8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/g/uniform_ones\")\n",
    "\n",
    "# Initialize model\n",
    "modelG=Sequential()\n",
    "\n",
    "# Add convolution layer\n",
    "modelG.add(Conv2D(32, (3,3), padding='same', input_shape = X_train.shape[1:],\n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG.add(Activation('sigmoid'))\n",
    "\n",
    "# Add pooling\n",
    "modelG.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add third convolution\n",
    "modelG.add(Conv2D(64, (3, 3), padding='same', \n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG.add(Activation('sigmoid'))\n",
    "\n",
    "# Add fourth convolution\n",
    "modelG.add(Conv2D(64, (3, 3), \n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG.add(Activation('sigmoid'))\n",
    "\n",
    "# Add second pooling\n",
    "modelG.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Add dense layer\n",
    "modelG.add(Flatten())\n",
    "modelG.add(Dense(512, \n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG.add(Activation('sigmoid'))\n",
    "\n",
    "# Add final dense layer \n",
    "modelG.add(Dropout(0.5))\n",
    "modelG.add(Dense(num_classes, \n",
    "                  kernel_initializer='random_uniform', \n",
    "                  bias_initializer='ones'))\n",
    "modelG.add(Activation('sigmoid'))\n",
    "\n",
    "# Add optimizer\n",
    "opt5=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "modelG.compile(loss='binary_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train, Y_train, \n",
    "        batch_size=batch_size)  \n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test, Y_test,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "modelG.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500*batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "Screenshots of the Tensorboard results are in my report and in tensorboard_screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
